{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1eceb1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e1a988e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "70237c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_from_gz(number):\n",
    "    number_str = str(number).zfill(4)\n",
    "    url = f\"https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed25n{number_str}.xml.gz\"\n",
    "    print(f\"Downloading {url} ...\")\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with gzip.GzipFile(fileobj=BytesIO(response.content)) as f:\n",
    "            context = ET.iterparse(f, events=('end',))\n",
    "            for event, elem in context:\n",
    "                if elem.tag == 'PubmedArticle':\n",
    "                    pmid_elem = elem.find('.//PMID')\n",
    "                    title_elem = elem.find('.//ArticleTitle')\n",
    "                    abstract_elem = elem.find('.//Abstract/AbstractText')\n",
    "                    \n",
    "                    pmid = pmid_elem.text if pmid_elem is not None else None\n",
    "                    title = title_elem.text if title_elem is not None else None\n",
    "                    abstract = abstract_elem.text if abstract_elem is not None else None\n",
    "                    \n",
    "                    articles.append((pmid, title, abstract))\n",
    "                    \n",
    "                    elem.clear()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "11392805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_articles():\n",
    "    all_articles = []\n",
    "    numbers = list(range(1, 1275))  # From 0001 to 1274\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(parse_articles_from_gz, number) for number in numbers]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            articles = future.result()\n",
    "            all_articles.extend(articles)\n",
    "    \n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb910da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = collect_all_articles()\n",
    "\n",
    "df = pd.DataFrame(all_articles, columns=[\"PMID\", \"ArticleTitle\", \"Abstract\"])\n",
    "print(f\"Total articles collected: {len(df)}\")\n",
    "\n",
    "\n",
    "df.to_csv('pubmed_articles_parallel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fef92565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse topic CD011134\n"
     ]
    }
   ],
   "source": [
    "from preprocess_utils import get_topics_pids_df\n",
    "\n",
    "training_data_paths = [\n",
    "    \"data/DATA2019/Training/DTA\",\n",
    "    \"data/DATA2019/Training/Intervention\",\n",
    "]\n",
    "\n",
    "initial_training_df = get_topics_pids_df(training_data_paths)\n",
    "\n",
    "test_data_paths = [\n",
    "    \"data/DATA2019/Testing/DTA\",\n",
    "    \"data/DATA2019/Testing/Intervention\",\n",
    "]\n",
    "\n",
    "initial_test_df = get_topics_pids_df(test_data_paths)\n",
    "\n",
    "initial_training_df['PID'] = initial_training_df['PID'].astype(str)\n",
    "initial_test_df['PID'] = initial_test_df['PID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "73f3be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1 = pd.read_csv('data/articles/articles_test.csv', sep=',')\n",
    "articles2 = pd.read_csv('data/articles/random_articles.csv', sep=',')\n",
    "\n",
    "articles1['PMID'] = articles1['PMID'].astype(str)\n",
    "articles2['PMID'] = articles2['PMID'].astype(str)\n",
    "\n",
    "articles = pd.concat([articles1, articles2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b8598fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(initial_training_df, articles, left_on='PID', right_on='PMID', how='left')\n",
    "train_df = train_df.dropna(subset=['ArticleTitle', 'Abstract'])\n",
    "\n",
    "train_df['relevance'] = 1\n",
    "train_df['article_title'] = train_df['ArticleTitle']\n",
    "train_df['abstract'] = train_df['Abstract']\n",
    "\n",
    "del train_df['PMID']\n",
    "del train_df['ArticleTitle']\n",
    "del train_df['Abstract']\n",
    "\n",
    "\n",
    "test_df = pd.merge(initial_test_df, articles, left_on='PID', right_on='PMID', how='left')\n",
    "test_df = test_df.dropna(subset=['ArticleTitle', 'Abstract'])\n",
    "\n",
    "test_df['relevance'] = 1\n",
    "test_df['article_title'] = test_df['ArticleTitle']\n",
    "test_df['abstract'] = test_df['Abstract']\n",
    "\n",
    "del test_df['PMID']\n",
    "del test_df['ArticleTitle']\n",
    "del test_df['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ccd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate misfitting articles, articles that are not in our training data\n",
    "\n",
    "def produce_irrelevant_citations(df):\n",
    "    possible_misfits = articles[\n",
    "        ~articles['PMID'].isin(df['PID']) & \n",
    "        articles['Abstract'].notna()\n",
    "    ]\n",
    "    \n",
    "    # Naively assume randomly picked topic is not relevant to sampled article\n",
    "    irrelevant = df.sample(n=len(df)).reset_index()\n",
    "    misfits = possible_misfits.sample(n=len(df)).reset_index()\n",
    "    \n",
    "    \n",
    "    irrelevant['article_title'] = misfits['ArticleTitle']\n",
    "    irrelevant['abstract'] = misfits['Abstract']\n",
    "    irrelevant['relevance'] = 0\n",
    "    irrelevant['PID'] = misfits['PMID']\n",
    "    \n",
    "    \n",
    "    out = pd.concat([df, irrelevant], ignore_index=True)\n",
    "    del out['index']\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "produce_irrelevant_citations(train_df).to_csv('data/train.csv', index=False)\n",
    "produce_irrelevant_citations(test_df).to_csv('data/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DevWorkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
